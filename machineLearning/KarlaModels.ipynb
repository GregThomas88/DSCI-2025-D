{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karla Jacobo  \n",
    "DSCI 8950  \n",
    "AI/ML Independent Study Report\n",
    "\n",
    "### Isolation Forest  \n",
    "\n",
    "I am going to start off by importing any necessary packages for running the Isolation Forest and Catboost models. I will also be importing the small dataset I have selected for this assignment. The dataset I have selected is one I got from Kaggle here: https://www.kaggle.com/datasets/himelsarder/cinema-hall-ticket-sales-and-customer-behavior?resource=download.\n",
    "Because it has data regarding movie goer behavior, it is a good fit for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticket_ID  Age  Ticket_Price Movie_Genre Seat_Type Number_of_Person  \\\n",
      "0     N4369   55         12.27      Comedy  Standard                7   \n",
      "1     B8091   35         19.02       Drama  Standard            Alone   \n",
      "2     V6341   55         22.52      Horror       VIP                3   \n",
      "\n",
      "  Purchase_Again  \n",
      "0             No  \n",
      "1            Yes  \n",
      "2             No  \n",
      "-----------------------------------\n",
      "MovieData table size: 1440 rows, 7 columns\n",
      "-----------------------------------\n",
      "There are 0/1440 rows missing data.\n",
      "-----------------------------------\n",
      "Counter({'object': 5, 'int64': 1, 'float64': 1})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "movieData = pd.read_csv('data/cinema_hall_ticket_sales.csv')\n",
    "allDataTypes = []\n",
    "\n",
    "print(f'{str(movieData.head(3))}')\n",
    "print('-----------------------------------')\n",
    "\n",
    "print(f'MovieData table size: {str(movieData.shape[0])} rows, {str(movieData.shape[1])} columns')\n",
    "\n",
    "print('-----------------------------------')\n",
    "\n",
    "null_data = movieData[movieData.isnull().any(axis=1)]\n",
    "print(f'There are {str(null_data.shape[0])}/{str(movieData.shape[0])} rows missing data.')\n",
    "\n",
    "print('-----------------------------------')\n",
    "dataTypes = movieData.dtypes.values\n",
    "for type in dataTypes:\n",
    "    allDataTypes.append(str(np.dtype(type)))\n",
    "\n",
    "print(collections.Counter(allDataTypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block above, we can see:  \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;- The table column headers  \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;- The dimensions of the table  \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;- The the types of data featured in the table with how often they appear  \n",
    "\n",
    "Now that we have the table imported and have verified there is no missing data, we can begin applying the Isolation Forest algorithm to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 anomalies detected out of 1440 rows of data\n",
      "\n",
      "Sample of anomalous data entries: \n",
      "      Age  anomaly_value\n",
      "79    18             -1\n",
      "164   18             -1\n",
      "193   18             -1\n",
      "-----------------------------------\n",
      "Sample of normal data entries: \n",
      "    Age  anomaly_value\n",
      "0   55              1\n",
      "1   35              1\n",
      "2   55              1\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 100 # This is the number of of trees to build\n",
    "contamination = 0.02 # This is the amount of contamination that is allowed\n",
    "\n",
    "ageData = movieData[['Age']]\n",
    "\n",
    "isolation_forest = IsolationForest(n_estimators=n_estimators,contamination=contamination)\n",
    "analysisData = isolation_forest.fit(ageData)\n",
    "\n",
    "analysisData = ageData.copy()\n",
    "analysisData['anomaly_value'] = isolation_forest.predict(ageData)\n",
    "anomalyData = analysisData[analysisData['anomaly_value'] != 1]\n",
    "normalData = analysisData[analysisData['anomaly_value'] == 1]\n",
    "\n",
    "print(f'{anomalyData.shape[0]} anomalies detected out of {analysisData.shape[0]} rows of data')\n",
    "print()\n",
    "print(f'Sample of anomalous data entries: \\n {anomalyData.head(3)}')\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(f'Sample of normal data entries: \\n {normalData.head(3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the code above two variables are named and given to the Isolation Forest model: n_estimators and contamination. N_estimators establishes how many trees are going to be made. Contamination establishes how much of our dataset we expect to contain anomalies. Anomalies are identified by the number -1 in the anomaly_value column. Normal values are the number 1. I set this one to 0.02 because if it were any lower or higher, either too many or no anomalies would be detected. I was able to determine that by manually tuning the model.\n",
    "\n",
    "As a result, we can see that 27 anomalies were detected in the Age column. Given the results of the model and the ages in the table, 18 year olds seem to be less likely to go see movies and are thus the outlier in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost  \n",
    "\n",
    "By utilizing CatBoost's regression capabilities, we can give it the Age and Ticket Price data from our movie dataset to see if there is any correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.4375000\ttotal: 1.28ms\tremaining: 1.28ms\n",
      "1:\tlearn: 0.3828125\ttotal: 2.77ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learn': {'RMSE': 0.3828125}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleData = movieData[['Age', 'Ticket_Price']]\n",
    "sample_size = int(sampleData.shape[0] * 0.2) # Getting training data from 20% of the entire dataset\n",
    "sampleData = sampleData.sample(sample_size, ignore_index=True)\n",
    "\n",
    "training_data = [] #Getting the data into an array format\n",
    "training_data.append(sampleData['Age'].values.tolist())\n",
    "training_data.append(sampleData['Ticket_Price'].values.tolist())\n",
    "\n",
    "evaluation_data = [] #Getting the data into an array format\n",
    "evaluation_data.append(movieData['Age'].values.tolist())\n",
    "evaluation_data.append(movieData['Ticket_Price'].values.tolist())\n",
    "\n",
    "training_labels = [0, 1]\n",
    "\n",
    "model = CatBoostRegressor(iterations=2)\n",
    "model.fit(training_data, training_labels)\n",
    "model.predict(evaluation_data)\n",
    "model.get_best_score()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block of code above, we can se that the original dataset had to be manipulated into an array of arrays, also known as a matrix. This is because when the model is being fit, that is the format it expects the training data and labels to be in. Once the data is tailored to how the model expects it, we can call the .fit(), .predict(), and the .get_best_score() methods.\n",
    "\n",
    "I chose to give the .fit() method a sample of two numerical columns from the original dataset. This sample is the size of 20% of the original dataset. Once the sample was selected, each column was turned into an array and added to the .fit() method along with numerical values for the training label definition. The .predict() method is run with the evaluation data. In this case I chose to use the entirety of the dataset. I am not sure if this is best practice. After that, I called the .get_best_score() method. This gets us the RMSE (Root Mean Square Error) value of the model, which is \"most commonly used measures for evaluating the quality of predictions\" (c3.ai). Generally, the lower the RMSE score is, the more accurate the better the model is. Given that the RMSE score here is 0.38, this particular model may have been overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources Used for Assistance\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html  \n",
    "https://www.datacamp.com/tutorial/isolation-forest  \n",
    "https://catboost.ai/docs/en/concepts/python-reference_catboostregressor  \n",
    "https://catboost.ai/docs/en/concepts/python-usages-examples\n",
    "https://c3.ai/glossary/data-science/root-mean-square-error-rmse/#:~:text=What%20is%20Root%20Mean%20Square,commonly%20used%20over%20standardized%20data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
